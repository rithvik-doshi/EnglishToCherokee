{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a5c282",
   "metadata": {},
   "source": [
    "# English to Cherokee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7b5ae",
   "metadata": {},
   "source": [
    "## Milestone Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df95152",
   "metadata": {},
   "source": [
    "### Team Members:\n",
    "Rithvik Doshi, Saisriram Gunturu, Ruihang (Henry) Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e4dec0",
   "metadata": {},
   "source": [
    "### Project Description\n",
    "\n",
    "We aim to create a model to translate English text to Cherokee. We're hoping to come up with an approach to this problem since Cherokee is an endangered language, and we can use the models we learned about in class specifically regarding machine translation to see how well we can do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba1ebd",
   "metadata": {},
   "source": [
    "### Approach\n",
    "We'll use the following data sources:\n",
    "- https://github.com/ZhangShiyue/ChrEn/tree/main/data\n",
    "- https://github.com/CherokeeLanguage/CherokeeEnglishCorpus/tree/master/corpus.aligned/en_chr\n",
    "\n",
    "Additionally, we will experiment with one of the following architectures/approaches to see what's the best way to translate from English to Cherokee:\n",
    "- https://github.com/lukysummer/Machine-Translation-Seq2Seq-Keras/tree/master/data\n",
    "- https://medium.com/@patrickhk/use-keras-to-build-a-english-to-french-translator-with-various-rnn-model-architecture-a374\n",
    "- https://github.com/LaurentVeyssier/Machine-translation-English-French-with-Deep-neural-Network/blob/main/machine_translation.ipynb\n",
    "- https://arxiv.org/pdf/2010.04791v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04930484",
   "metadata": {},
   "source": [
    "### Project Plan:\n",
    "\n",
    "The project will consist of the following phases:\n",
    "1. EDA / Data Loading\n",
    "    a. Concatenating as many data sources as possible to get as big of a corpus as we can\n",
    "    b. Split data into training, testing and validation sets\n",
    "2. Model Developemnt\n",
    "    a. Finalize Model Selection and Architecture and build in Pytorch\n",
    "3. Model Training\n",
    "    a. Use available SCC GPUs to train the model.\n",
    "4. Model Validation\n",
    "    a. Validation sentences should give us a metric of accuracy.\n",
    "5. Model Testing\n",
    "    a. Use ChrEn model to translate back to English to see how we did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b49d13",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c4bbc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c87acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import shuffle, seed, choice\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split,Dataset,DataLoader,TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "import pickle\n",
    "from statistics import mean\n",
    "from torchtext.vocab import GloVe\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6dae3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb55accf-9186-4701-886d-e4b41920417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2e4418-8c6b-44f6-b5dd-475fe274f088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae46f3",
   "metadata": {},
   "source": [
    "# Data Loading and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9bc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = \"./CherokeeEnglishCorpus/corpus.aligned/en_chr/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0698a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_for(language=\".en\"):\n",
    "    \"\"\"\n",
    "    Load the input for the given language\n",
    "    :param language: the language of the input (\".en\" for English and \".chr\" for Cherokee)\n",
    "    :return: the input\n",
    "    \"\"\"\n",
    "    # Get all .en files in the directory\n",
    "    file_list = [file for file in os.listdir(data_dir) if file.endswith(language)]\n",
    "\n",
    "    # Initialize the empty array for the input\n",
    "    lines_array = []    # structure: [lines in the document]\n",
    "\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                lines_array.append(line.strip())\n",
    "\n",
    "    return lines_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676d854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = load_input_for(\".en\")\n",
    "cherokee_sentences = load_input_for(\".chr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25781fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107168 107168\n"
     ]
    }
   ],
   "source": [
    "print(len(english_sentences), len(cherokee_sentences))  # should match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780a18e",
   "metadata": {},
   "source": [
    "## Data pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b76421",
   "metadata": {},
   "source": [
    "### Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d46ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henryliu/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:132: UserWarning: A NumPy version >=1.21.6 and <1.28.0 is required for this version of SciPy (detected version 1.21.5)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(x):\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    return x_tk.texts_to_sequences(x), x_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922d994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 1, 7, 3, 8, 1, 9, 2, 1, 10], [2, 3, 11, 12, 4, 13, 5, 2, 4, 14, 5]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our tokenize()\n",
    "test_text = [\"In the beginning God created the heavens and the earth.\",\n",
    "             \"And God said, Let there be light: and there was light.\"]  # just 2 short sentences from our data\n",
    "test_text_tokenized, test_tokenizer = tokenize(test_text)\n",
    "\n",
    "test_text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bcf893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'god': 3,\n",
       " 'there': 4,\n",
       " 'light': 5,\n",
       " 'in': 6,\n",
       " 'beginning': 7,\n",
       " 'created': 8,\n",
       " 'heavens': 9,\n",
       " 'earth': 10,\n",
       " 'said': 11,\n",
       " 'let': 12,\n",
       " 'be': 13,\n",
       " 'was': 14}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d823b",
   "metadata": {},
   "source": [
    "We can see that keras has already taken into account of capital/lowercased letter and punctuations. So we don't have to.\n",
    "\n",
    "Apply tokenizer on our input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df41e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences_tokenized, english_tokenizer = tokenize(english_sentences)\n",
    "cherokee_sentences_tokenized, cherokee_tokenizer = tokenize(cherokee_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9eb904b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size = 20763, Cherokee vocab size = 72759\n"
     ]
    }
   ],
   "source": [
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "cherokee_vocab_size = len(cherokee_tokenizer.word_index)\n",
    "print(\"English vocab size = {}, Cherokee vocab size = {}\".format(english_vocab_size, cherokee_vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4430d8d8",
   "metadata": {},
   "source": [
    "### Padding\n",
    "Truncate all sentences into equal length for our input: pad to the max length, leave trailing 0 (post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c2c68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences   # for Apple Sillicon is \"keras_preprocessing\". Otherwise \"keras.preprocessing\"\n",
    "def pad(x):\n",
    "    length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "434e9988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  1,  7,  3,  8,  1,  9,  2,  1, 10,  0],\n",
       "       [ 2,  3, 11, 12,  4, 13,  5,  2,  4, 14,  5]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing padding function:\n",
    "test_text_padded = pad(test_text_tokenized)\n",
    "test_text_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3827612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding to input:\n",
    "english_sentences_padded = pad(english_sentences_tokenized)\n",
    "cherokee_sentences_padded = pad(cherokee_sentences_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da416f",
   "metadata": {},
   "source": [
    "### Write function to map logits back to token label\n",
    "Function to convert predictions (a bunch of probability) back to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0dc2fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    idx_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    idx_to_words[0] = '<PAD>'\n",
    "    return ' '.join([idx_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f308d7",
   "metadata": {},
   "source": [
    "### Make Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d02f2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Basic_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # return a pair x,y at the index idx in the data set\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1148719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(english_sentences_padded, cherokee_sentences_padded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the train data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Basic_Dataset(X_train, y_train)\n",
    "val_dataset = Basic_Dataset(X_val, y_val)\n",
    "test_dataset = Basic_Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c639226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For torch models:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Or a loader with all data:\n",
    "all_loader = DataLoader(Basic_Dataset(english_sentences_padded, cherokee_sentences_padded), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a336ea8",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a337165",
   "metadata": {},
   "source": [
    "## First Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b548733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng2chrvenv",
   "language": "python",
   "name": "eng2chrvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
